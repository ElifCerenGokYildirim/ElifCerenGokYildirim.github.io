<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Continual Learning with Informative Samples: An Empirical Evaluation of Coreset Strategies</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          line-height: 1.6;
          margin: 0;
          padding: 0;
          background-color: #f4f4f4;
          color: #333;
      }
      .container {
          max-width: 800px;
          margin: 20px auto;
          padding: 20px;
          background-color: #fff;
          box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
          border-radius: 5px;
      }
      h1 {
          font-size: 30px;
          margin-top: 0;
          text-align: center;
          padding: 20px 0;
          background-color: #d0e8f2;
          color: #2c3e50;
          border-radius: 5px;
      }
      h2 {
          font-size: 24px;
          margin-top: 30px;
          color: #555;
      }
      h3 {
          font-size: 20px;
          margin-top: 20px;
          color: #666;
      }
      p {
          margin: 10px 0;
          color: #444;
      }
      ul, ol {
          margin: 10px 0;
          padding-left: 20px;
      }
      a {
          color: #777;
          text-decoration: none;
      }
      a:hover {
          text-decoration: underline;
      }
      .colored1 {
          color: #ff3380;
      }
      .colored2 {
          color: #77ab59;
      }
      .colored3 {
          color: #f44336;
      }
      .center {
          text-align: center;
      }
      .author-section {
          display: flex;
          flex-wrap: wrap;
          align-items: center;
      }
      .author-section h3 {
          margin-right: 5px;
          font-size: 20px;
          color: #444;
      }
      .author-section p {
          margin: 0;
          font-size: 20px;
      }
      .author-section p:not(:last-child)::after {
          content: " ";
          margin-right: 5px;
      }
      img {
          max-width: 100%;
          height: auto;
          margin-top: 15px;
          margin-bottom: 15px;
          border-radius: 5px;
          box-shadow: 0 0 5px rgba(0, 0, 0, 0.1);
      }
      table {
          width: 100%;
          border-collapse: collapse;
          margin: 20px 0;
      }
      th, td {
          border: 1px solid #ddd;
          padding: 12px;
          text-align: left;
          vertical-align: top;
      }
      th {
          background-color: #f2f2f2;
          text-align: center;
      }
  </style>
</head>
<body>
  <div class="container">
      <h1>Continual Learning with Informative Samples: An Empirical Evaluation of Coreset Strategies</h1>

      <div class="author-section">
          <h3>Authors:</h3>
          <p><strong>Elif Ceren Gok Yildirim</strong>,</p>
          <p><strong>Onur Yildirim</strong>,</p>
          <p><strong>Joaquin Vanschoren</strong></p>
      </div>

      <h2>Background</h2>
      <p>
          Continual Learning (CL) aims to enable models to learn sequentially from streaming data, retaining previously acquired knowledge while adapting to new tasks, a duality known as the <strong>stability-plasticity</strong> balance. This balance is critical for  mimicking human-like learning, where accumulated knowledge is preserved (stability) yet flexibly updated with novel experiences (plasticity). There is big line of research that focus on achieving better stability-plasticity tradeoff by building and proposing new methods to literature.
      </p>

      <h2>Motivation</h2>
      <p>
          While these methods improve performance, they share a common  assumption: all training samples are equally valuable and must be exhaustively utilized. By default, this standardized practice prioritizes plasticity (integrating new information) at the risk of destabilizing learned representations, as redundant or noisy samples may overwrite critical prior knowledge.
          This ‚Äùlearn-it-all‚Äù paradigm diverges from human learning efficiency since, as humans, we are initially exposed to vast amounts of information but intuitively filter and prioritize them, focusing on key experiences (e.g. clear and novel examples) that enrich our understanding while disregarding redundant details.
      </p>
       <img src="datadietcl.png" alt="Teaser image CL with coreset" class="center">

      <h2>The Overall Game Plan</h2>
      <p>
          In our approach, we put traditional CL methods to the test by comparing their performance when trained on the full dataset versus when they're trained on a carefully selected, informative subset of samples (<strong>coreset</strong>).
      </p>

      <p>
          We structured our training into two distinct phases:
      </p>
      <ul>
          <li>
              <strong>Warm-Up Phase:</strong> The CL model undergoes initial, partial training. This phase is key because it lets us analyze the model's behavior and data representations, which in turn helps us pick out the most informative samples.
          </li>
          <li>
              <strong>Learning Phase:</strong> Once the warm-up is complete, we switch to training on just the selected subset of samples. This phase is much longer and is where the model truly refines its knowledge.
          </li>
      </ul>
      <p>
          Note that the warm-up phase is generally much shorter than the learning phase since it's only long enough for the model to "warm up" and provide the insights needed for effective sample selection.
      </p>

      <h3>CL Baselines</h3>
      <p>
          Below is a quick comparison of the CL baselines we used in our work:
      </p>
      <table>
        <thead>
          <tr>
            <th>Architecture-based</th>
            <th>Memory-based</th>
            <th>Regularization-based</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>DER, FOSTER, MEMO</td>
            <td>iCaRL, ER</td>
            <td>LwF</td>
          </tr>
          <tr>
            <td>
              Dynamically expands the model's representation space by adding new features for new tasks.
            </td>
            <td>
              Holds a memory buffer from previous tasks and keeps learning from them along with new tasks.
            </td>
            <td>
              Regularizes the model's parameters when learning new tasks to retain previously acquired knowledge.
            </td>
          </tr>
        </tbody>
      </table>

      <h3>Coreset Baselines</h3>
      <p>
          Below is a quick comparison of the Coreset baselines we used in our work:
      </p>

      <img src="coreset.png" alt="Baselines of coreset" class="center">

      <h2>Findings and Insights</h2>
        <p>
            We observed a significant improvement in continual learning performance when training with <strong>coreset samples</strong> instead of full datasets. We found out that this enhancement comes from achieving optimal <strong>stability-plasticity tradeoff</strong>.
        </p>

        <p>
            While training with all samples often boosts performance on the current task, it tends to forget earlier tasks. In contrast, using a selective subset (coreset) of samples helps retain prior knowledge better, even if it sacrifices on current task performance.
        </p>

        <p>
            The heatmap below illustrates <strong>accuracy across tasks after each learning session</strong> on Split-CIFAR10. It compares full-sample training with the best-performing coreset strategy.
        </p>

        <p class="center">
            <img src="heatmap.png" alt="Task-wise accuracy heatmap comparison" class="center" style="max-width: 70%;">
        </p>

        <p>
            As seen in the figure, <strong>coreset training reduces forgetting</strong>, preserving earlier knowledge more effectively.
        </p>

        <h3>Why Does It Work: A Look Into Representations</h3>
        <p>
            To better understand <em>why</em> this happens, we visualized the learned feature space using t-SNE plots of DER‚Äôs representations on Split-CIFAR10.
        </p>

        <p class="center">
            <img src="tsne.png" alt="t-SNE with 20% coreset">
        </p>

        <p>
            With  <strong>20%</strong> of the data, coreset-trained models form <strong>well-separated class clusters</strong>, suggesting clearer, less entangled representations.
        </p>

        <p>
            As we increased the coreset fraction to <strong>80%</strong>, and then to the full dataset, the class clusters became <strong>less distinct</strong>:
        </p>


        <p>
            We also quantified this observation by calculating <strong>inter-class distances</strong> in the t-SNE-embedded space:
        </p>

        <ul>
            <li><strong>20% coreset</strong>: 11.68</li>
            <li><strong>80% coreset</strong>: 10.71</li>
            <li><strong>Full dataset</strong>: 10.67</li>
        </ul>

        <p>
            This confirms our visual findings: <strong>more data does not always lead to better representations or ensure better learning</strong>. Sometimes, it blurs the boundaries between classes, increasing the risk of forgetting past tasks specially in CL.
        </p>

        <h2>When Coresets Don't Help</h2>
          <p>
              While coreset-based training improves many CL methods, there are notable exceptions. Methods like <strong>FOSTER</strong> and <strong>LwF</strong> (which relies solely on regularization) perform better when trained with the full dataset. These approaches seem to require the complete data distribution to operate effectively.
          </p>
          <p>
              We provide detailed insights and analysis for these exceptions in the full paper, including possible explanations and ablation studies.
          </p>

      <p>
    Since I don't want to bore you with nitty-gritty details, I invite you to check out <a href="https://arxiv.org/pdf/2410.17715">our paper</a>.
    We would be happy if you want to talk about this work, so please feel free to reach out to us üòä.
      </p>


  </div>
</body>
</html>
