<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Variational Autoencoders (VAEs)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
        }

        h1 {
            font-size: 30px;
            margin-top: 0;
            color: #333;
            text-align: center;
            padding: 20px 0;
            background-color: #b3d5a4;
            color: #7a848d;
            border-radius: 5px;
        }

        h1 a {
            color: inherit;
            text-decoration: none;
        }

        h1 a:hover {
            text-decoration: underline;
        }

        h2 {
            font-size: 24px;
            margin-top: 30px;
            color: #555;
        }

        h3 {
            font-size: 20px;
            margin-top: 20px;
            color: #777;
        }

        img {
            max-width: 100%;
            height: auto;
            margin-top: 15px;
            margin-bottom: 15px;
            border-radius: 5px;
            box-shadow: 0 0 5px rgba(0, 0, 0, 0.1);
        }

        ul {
            margin-top: 10px;
            margin-bottom: 10px;
            padding-left: 20px;
        }

        p {
            margin-top: 10px;
            margin-bottom: 10px;
            color: #444;
        }

        a {
            color: #777;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }
        .author-section {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
        }

        .author-section h3 {
            margin-right: 5px;
            font-size: 18px;
        }

        .author-section p {
            margin: 0;
            font-size: 12px;
        }

        .author-section p:not(:last-child)::after {
            content: " ";
            margin-right: 5px;
        }

        .colored1 {
            color: #2986cc;
        }
        .colored2 {
            color: #e06666;
        }
        .colored3 {
            color: #df912c;
        }
        .colored4 {
            color: #876c94;
        }

    </style>
</head>
<body>
    <div class="container">
        <h1>Variational Autoencoders (VAEs)</h1>

        <article class="blog-post">
            <p>Variational Autoencoders are a type of generative model, designed to learn a representation of data and generate new data points similar to the original data.</p>
            <p>They are built upon the architecture of traditional autoencoders, consisting of two main parts: the <strong><span class="colored1">Encoder</span></strong> and the <strong><span class="colored2">Decoder</span>.</strong></p>

            <h2>Architecture of Autoencoders</h2>
            <ul>
                <li><strong><span class="colored1">Encoder</span>:</strong> The encoder compresses the input data into a latent vector (a lower-dimensional representation).</li>
                <li><strong><span class="colored2">Decoder</span>:</strong> The decoder takes this latent vector and reconstructs the original input data from it.</li>
            </ul>
            <p><strong>Small note:</strong> Despite the decoder being responsible for generating the output, the encoder plays a crucial role in producing a meaningful latent vector. If the latent vector is not well-structured, the decoder will generate meaningless outputs.</p>
            <img src="/ai-concepts/vae/1.png" alt="Encoder and Decoder">

            <h2>Variational Autoencoder vs. Autoencoder</h2>
            <p>The primary distinction between an <span class="colored3">Autoencoder (AE)</span> and a <span class="colored4">Variational Autoencoder (VAE)</span> lies in how the latent space is structured:</p>
            <ul>
                <li><strong><span class="colored3">Autoencoder (AE)</span>:</strong> An AE simply learns to map inputs to latent vectors and then back to outputs without any constraints on the latent space just like NN without any regularization. The embeddings of inputs can be very far apart from each other or not. This can lead to a disorganized latent space, making generation of new data challenging.</li>
            </ul>
            <p>The embeddings of training inputs are learned during training. Without regularizing the space of the embeddings we might end up with a huge space with limited information.</p>
            <img src="/ai-concepts/vae/2.png" alt="Encode Training">
            <p>During test time, we might end up with embeddings where we donâ€™t have information in this big space.</p>
            <img src="/ai-concepts/vae/3.png" alt="Encoder Testing">
            <p>And.. During the test time in the decoding part, we might end up with meaningless outputs.</p>
            <div style="text-align: center;">
                <img src="/ai-concepts/vae/4.png" alt="Decoder Test" style="width: 75%; height: auto;">
            </div>
            <ul>
                <li><strong><span class="colored4">Variational Autoencoder (VAE)</span>:</strong> A VAE imposes a structure on the latent space through regularization. Specifically, it constrains the latent vectors to follow a normal distribution N(0,I). This is achieved by introducing a latent loss (often referred to as the Kullback-Leibler divergence) during training, which forces the latent vectors to conform to this distribution.</li>
            </ul>
            <p><strong>A very high level representation for loss of the VAE:</strong></p>
            <p>Loss = Reconstruction Loss + Latent Loss
            <p><strong>A very high level representation for loss for the AE:</strong></p>
            <p>Loss = Reconstruction Loss</p>

            <h2>Why Regularization is Necessary</h2>
            <p>The regularization in VAEs serves several purposes:</p>
            <ul>
                <li><strong>Controlled Latent Space:</strong> By regularizing the latent space, the VAE ensures that similar inputs have similar latent representations. This structured latent space allows for more meaningful and coherent generation of new data.</li>
                <li><strong>Generalization:</strong> During training, the VAE learns to map inputs to a specific, controlled space. As a result, when given new, unseen inputs, the VAE can project these inputs into this learned space and produce meaningful outputs. Without regularization (as in AEs), the latent vectors could be scattered randomly, leading to meaningless outputs for new inputs.</li>
            </ul>
            <p>This regularization will actually force that the coming embedding from test input will be placed within this constrained space with this way the decoder part hopefully will be able to generate more meaningful outputs.</p>
            <div style="text-align: center;">
                <img src="/ai-concepts/vae/5.png" alt="why vae">
            </div>
        </article>
    </div>
</body>
</html>
